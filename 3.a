print("\n--- Problem 3.a Results (5 Features with L2 Regularization) ---")

# Setup (using Standardization from 2.a)
X_train_reg_3a = X_train_s_1a
X_test_reg_3a = X_test_s_1a
y_train_reg_3a = y_train_1a
y_test_reg_3a = y_test_1a
initial_theta_reg_3a = np.zeros(X_train_reg_3a.shape[1])

# Run Regularized Gradient Descent
best_theta_reg_3a, train_loss_hist_reg_3a, val_loss_hist_reg_3a = gradient_descent_regularized(
    X_train_reg_3a, y_train_reg_3a, initial_theta_reg_3a, ALPHA_SCALED, ITERATIONS_SCALED,
    LAMBDA_REG, X_test_reg_3a, y_test_reg_3a
)

# Final Evaluation (Validation loss is UNREGULARIZED)
final_validation_loss_reg_3a = compute_loss(X_test_reg_3a, y_test_reg_3a, best_theta_reg_3a)

print(f"L2 Regularization (lambda={LAMBDA_REG}) Val Loss: {final_validation_loss_reg_3a:.2f}")
print(f"Baseline Scaled Val Loss (2.a, Std): {final_validation_loss_s_1a:.2f}")

# Plotting
plot_loss(train_loss_hist_reg_3a, val_loss_hist_reg_3a,
          f"3.a) Loss History (5 Features, {BEST_SCALING} + L2 Reg. $\lambda={LAMBDA_REG}$)",
          ALPHA_SCALED, ITERATIONS_SCALED)
